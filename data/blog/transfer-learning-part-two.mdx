---
title: 'Transfer Learning - Part 2'
date: '2024-05-29'
tags: ['machine learning', 'deep learning', 'transfer-learning']
draft: true
summary: 'Introduction to transfer learning '
images: ['https://file.kiarashs.ir/PicGo/transfer%20learning%202.png']
authors: ['default']
---



In this article, I want to introduce some other basics of transfer learning, which are essential to use transfer learning techniques in your projects.




# Notation

At first, we will present the mathematical definition of the basic concepts related to transfer learning, as well as the symbols that are used to express these concepts, so that we can provide a precise mathematical definition of the transfer learning process based on them.



## Domain

Domain consists of two components, a **feature space** $\mathscr{X}$, and a **marginal probability distribution**:
$$
P(X), where\;X = \{x_1, x_2, \ldots, x_n\} \in{\mathscr{X}}
$$

For example, if our learning task is *document classification*, and each term is taken as a binary feature, then $\mathscr{X}$ is the space of all term vectors, $x_i$ is the $i^{th}$ term vector corresponding to some documents, and $X$ is a particular learning sample.

In general, **if two domains are different**, then they may have different feature spaces or different marginal distributions.



## Task

Given a specific domain $D = \{\mathscr{X},\ P(X)\}$, a task consists of two parts:

1. A label space $\mathcal{Y}$, for each $x_i$ in the domain, to predict its corresponding label $y_i, where\ y_i \in{\mathcal{Y}}$
2. An objective predictive function $f(.)$ (denoted by $\mathcal{T}= \{\mathcal{Y}, f(.)\}$); $f(x)$ can be written as $P(y|x)$.

**If two tasks are different**, then they may have different label spaces or different conditional distributions.



Considering one source domain data $D_S$, and one target domain data $D_T$:

* $D_S = \{(x_{S_{1}}, y_{S_{1}}), \ldots, (x_{S_{n_{S}}}, y_{S_{n_{S}}})\}$, where $x_{S_{i}} \in \mathcal{X_S}$ the data instance and $y_{S_{i}} \in \mathcal{Y_S}$ the corresponding class label.
* Similarly, the target domain data is $D_T = \{(x_{T_{1}}, y_{T_{1}}), \ldots, (x_{T_{n_{T}}}, y_{T_{n_{T}}})\}$, where $x_{T_{i}} \in \mathcal{X_T}$  and $y_{T_{i}} \in \mathcal{Y_T}$ is the corresponding output.
* In most cases: $0 \leq n_T \ll n_S$ .



## Summary of notations used

The below table shows a summary of notations that are used in transfer learning:

| **Notation**   | **Description**          | **Notation** | **Description**          |
| -------------- | ------------------------ | ------------ | ------------------------ |
| $\mathcal{X}$  | Input feature space      | $P(X)$       | Marginal distribution    |
| $\mathcal{Y}$  | Label space              | $P(Y|X)$     | Conditional distribution |
| $\mathcal{T}$  | Predictive learning task | $P(Y)$       | Label distribution       |
| $Subscript\ S$ | Denotes source           | $D_S$        | Source domain data       |
| $Subscript\ T$ | Denotes target           | $D_T$        | Target domain data       |



# Definition of transfer learning

Given a source domain $D_S$ and learning task $T_S$, a target domain $ùíü_ùëá$ and learning task $$T_T$$, transfer learning aims to help improve the learning of the target predictive function $f_T(.)$ in $D_T$ using the knowledge in $D_S$ and $T_S$, where:

* $D_S \neq D_T$ , or
* $T_S \neq T_T$



